{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Libraries for Data Loading and Processing\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Pytorch Libraries for CNN Classification\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch import optim\n",
    "import torch \n",
    "\n",
    "# Custom Libraries for Performance Evaluation\n",
    "from evaluate_classification import EvaluateBinaryClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Random Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\SentimentData\\\\Hate\\\\'\n",
    "fins_train = ['eastasian_hate_sub_train.csv']\n",
    "fins_test = ['eastasian_hate_sub_test.csv']\n",
    "track = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply only this preprocessing because our data is already preprocessed\n",
    "def cleanNonAscii(text):\n",
    "    '''\n",
    "    Remove Non ASCII characters from the dataset.\n",
    "    Arguments:\n",
    "        text: str\n",
    "    returns: \n",
    "        text: str\n",
    "    '''\n",
    "    return ''.join(i for i in text if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; i can ’ t say what i want to say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; trust me . as hong konger , i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i ’ ve been living in china during the corona ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>those who like blaming &lt;hashtag&gt; chinese virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;hashtag&gt; china virus &lt;/hashtag&gt; china should ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  <user> <user> i can ’ t say what i want to say...\n",
       "1      1  <user> <user> trust me . as hong konger , i ca...\n",
       "2      0  i ’ ve been living in china during the corona ...\n",
       "3      1  those who like blaming <hashtag> chinese virus...\n",
       "4      1  <hashtag> china virus </hashtag> china should ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(BASE+fins_train[track])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "label      \n",
       "0      3120\n",
       "1      3116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train['text'].values, df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>with such damning evidence , the silence of &lt;u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; does &lt;allcaps&gt; cdc &lt;/allcaps&gt; ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;url&gt; &lt;hashtag&gt; chinazi virus &lt;/hashtag&gt; keeps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; 2 4 hrs later . unimpressed . &lt;hashtag&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>rt &lt;user&gt; make sure you are prepared with the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  with such damning evidence , the silence of <u...\n",
       "1      0  <user> <user> does <allcaps> cdc </allcaps> ha...\n",
       "2      1  <url> <hashtag> chinazi virus </hashtag> keeps...\n",
       "3      0  <user> 2 4 hrs later . unimpressed . <hashtag>...\n",
       "4      0  rt <user> make sure you are prepared with the ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(BASE+fins_test[track])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_test['text'].values, df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<user> <user> i can  t say what i want to say about scaramucci but i will say how stupid he sounds , especially when he prefaces every ridiculous statement with ,  at the end of the day .  dumb . <hashtag> wuhan virus </hashtag> <hashtag> chinese coronavirus </hashtag>',\n",
       " 'with such damning evidence , the silence of <user> and the international community to call out <hashtag> china </hashtag>  s irresponsible actions that are threatening the world is not only baffling but points to something nefarious altogether . <hashtag> wuhan coronavirus </hashtag> <hashtag> china virus </hashtag> <hashtag> covid19 </hashtag> <url>')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data suitable for model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "num_words = 100000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = max(map(lambda x: len(x),xtrain))\n",
    "xtrain = pad_sequences(xtrain, maxlen=maxlen)\n",
    "\n",
    "xtest = tokenizer.texts_to_sequences(X_test)\n",
    "xtest = pad_sequences(xtest, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  16,   7,   1],\n",
       "       [  0,   0,   0, ...,  88,   1,  10],\n",
       "       [  0,   0,   0, ...,   6,   4,   1],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   2,   1,  10],\n",
       "       [  0,   0,   0, ...,  13, 176,  10],\n",
       "       [  0,   0,   0, ...,   4,   1,   5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    # Preprocessing parameeters\n",
    "    seq_len: int = xtrain.shape[1]\n",
    "    num_words: int = 100000\n",
    "   \n",
    "    # Model parameters\n",
    "    embedding_size: int = 64\n",
    "    out_size: int = 32\n",
    "    stride: int = 2\n",
    "   \n",
    "    # Training parameters\n",
    "    epochs: int = 13\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    \n",
    "params=Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nn.ModuleList Holds submodules in a list.\n",
    "ModuleList can be indexed like a regular Python list, \n",
    "but modules it contains are properly registered, \n",
    "and will be visible by all Module methods.\n",
    "\n",
    "'''\n",
    "class CnnTextClassifier(nn.ModuleList):\n",
    "    \n",
    "\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # set paremeters related to text preparation\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.emb_size = params.embedding_size\n",
    "        self.stride = params.stride\n",
    "        \n",
    "        # define dropouts\n",
    "        self.dropout50 = nn.Dropout(0.5)\n",
    "        self.dropout20 = nn.Dropout(0.2)\n",
    "        \n",
    "        ### now define parameters for CNN\n",
    "        \n",
    "        # define kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "        \n",
    "        # define output size of each kernel\n",
    "        self.kout_size = params.out_size\n",
    "        \n",
    "        # define number of stride for each kernel\n",
    "        self.stride = params.stride\n",
    "        \n",
    "        # define embedding layer\n",
    "        self.embedding = nn.Embedding(self.num_words+1, self.emb_size, padding_idx=0)\n",
    "        \n",
    "        # define convolution layers\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.kout_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.kout_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.kout_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.kout_size, self.kernel_4, self.stride)\n",
    "        \n",
    "        # define pooling layers\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "        \n",
    "        # define fully connected layer\n",
    "        self.fc = nn.Linear(self.in_size_features_fc(), 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # feed the numerical representation x of a text to embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # apply convolution layer 1\n",
    "        x1 = self.conv_1(x) # batch size, number of channels, height, width\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "        \n",
    "        # apply convolution layer 2\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu(x2)\n",
    "        x2 = self.pool_2(x2)\n",
    "        \n",
    "        # apply convolution layer 3\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "        \n",
    "        # apply convolution layer 4\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "        \n",
    "        # now we concatenate the output of each convolution layer\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "        \n",
    "        # now we pass the flattened vector 'union' to the fully connected layer\n",
    "        out = self.fc(union)\n",
    "        \n",
    "        # apply drouptou\n",
    "        out = self.dropout20(out)\n",
    "        \n",
    "        # apply sigmoid activation to get a probability distribution\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out.squeeze()\n",
    "        \n",
    "        \n",
    "    def in_size_features_fc(self):\n",
    "        '''\n",
    "        Calculates the number of output features after Convolution + Max pooling \n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        '''\n",
    "        # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "        out_conv_1 = ((self.emb_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "      \n",
    "        # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "        out_conv_2 = ((self.emb_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "      \n",
    "        # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "        out_conv_3 = ((self.emb_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "      \n",
    "        # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "        out_conv_4 = ((self.emb_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "      \n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.kout_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapper(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset mapper objects for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DatasetMapper(xtrain, y_train)\n",
    "test = DatasetMapper(xtest, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise dataloaders for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "loader_test = DataLoader(test, batch_size=params.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CnnTextClassifier(params)\n",
    "optimizer = optim.RMSprop(cnn_model.parameters(), lr=params.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "   \n",
    "    # Gets frequency  of true positives and true negatives\n",
    "    # The threshold is 0.5\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred >= 0.5) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred < 0.5) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    # Return accuracy\n",
    "    return (true_positives+true_negatives) / len(grand_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<----epoch 0\n",
      "accuracy 0.5829057087876844 -------->\n",
      "<----epoch 1\n",
      "accuracy 0.7171263630532393 -------->\n",
      "<----epoch 2\n",
      "accuracy 0.7830339961513791 -------->\n",
      "<----epoch 3\n",
      "accuracy 0.8337075048107762 -------->\n",
      "<----epoch 4\n",
      "accuracy 0.8521488133418859 -------->\n",
      "<----epoch 5\n",
      "accuracy 0.8781270044900578 -------->\n",
      "<----epoch 6\n",
      "accuracy 0.8904746632456703 -------->\n",
      "<----epoch 7\n",
      "accuracy 0.8968890314304041 -------->\n",
      "<----epoch 8\n",
      "accuracy 0.8960872354073124 -------->\n",
      "<----epoch 9\n",
      "accuracy 0.8920782552918538 -------->\n",
      "<----epoch 10\n",
      "accuracy 0.895445798588839 -------->\n",
      "<----epoch 11\n",
      "accuracy 0.9021808851828095 -------->\n",
      "<----epoch 12\n",
      "accuracy 0.8938422065426556 -------->\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(params.epochs):\n",
    "    # set the model in training mode\n",
    "    cnn_model.train()\n",
    "    predictions = []\n",
    "    print('<----epoch', epoch)\n",
    "    \n",
    "    # start training the batches\n",
    "    for x_batch, y_batch in loader_train:\n",
    "        y_batch = y_batch.type(torch.FloatTensor) # changing datatype to float tensor\n",
    "        x_batch = x_batch.type(torch.LongTensor)\n",
    "        y_pred = cnn_model(x_batch) # feed to the model\n",
    "        loss = binary_cross_entropy(y_pred, y_batch) # calcualte loss\n",
    "        optimizer.zero_grad() # clearn gradiants\n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # update parameters based on gradients\n",
    "        predictions += list(y_pred.detach().numpy())\n",
    "    \n",
    "    # evaluate for one epoch\n",
    "    accuracy = calculate_accuray(y_train, predictions)\n",
    "    print('accuracy', accuracy, '-------->')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader_test):\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    # Starst evaluation phase\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader_test:\n",
    "            x_batch = x_batch.type(torch.LongTensor)\n",
    "            y_pred = model(x_batch)\n",
    "            predictions += list(y_pred.detach().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7115384615384616\n"
     ]
    }
   ],
   "source": [
    "test_predictions = evaluation(cnn_model, loader_test)\n",
    "test_accuracy = calculate_accuray(y_test, test_predictions)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluateBinaryClassification Object Created\n",
      "\n",
      "Total Samples\t780\n",
      "Positive Samples\t380\n",
      "Negative Samples\t400\n",
      "True Positive\t265\n",
      "True Negative\t290\n",
      "False Positive\t110\n",
      "False Negative\t115\n",
      "Accuracy\t0.7115384615384616\n",
      "Precision\t0.7066666666666667\n",
      "Recall\t0.6973684210526315\n",
      "F1 Measure\t0.7019867549668874\n",
      "Cohen Kappa Score\t0.42250740375123397\n",
      "Area Under Curve\t0.7111842105263156\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72       400\n",
      "           1       0.71      0.70      0.70       380\n",
      "\n",
      "    accuracy                           0.71       780\n",
      "   macro avg       0.71      0.71      0.71       780\n",
      "weighted avg       0.71      0.71      0.71       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions\n",
    "ebc = EvaluateBinaryClassification(gnd_truths = y_test, predictions = [int(round(y)) for y in test_predictions])\n",
    "print(ebc.get_full_report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
