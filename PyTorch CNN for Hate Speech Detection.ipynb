{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9500c71",
   "metadata": {},
   "source": [
    "# PyTorch CNN for Hate Speech Detection\n",
    "\n",
    "PyTorch Implementation of the following Paper \n",
    "\n",
    "Misogynistic Tweet Detection: Modelling CNN with Small Datasets\n",
    "\n",
    "Cite:\n",
    "\n",
    "@inproceedings{bashar2018cnn, title={Misogynistic Tweet Detection: Modelling CNN with Small Datasets}, author={Bashar, Md Abul and Nayak, Richi and Suzor, Nicolas and Weir, Bridget}, booktitle={The 16th Australasian Data Mining Conference}, year={2018} }\n",
    "\n",
    "Keras Implementation is available at the following link\n",
    "\n",
    "https://github.com/mdabashar/CNN_for_Misogynistic_Tweet_Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5b4ad",
   "metadata": {},
   "source": [
    "#### 0.0 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1499739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from softmax import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a853b2",
   "metadata": {},
   "source": [
    "#### 0.1 Initialise Random Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97add15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b253b69",
   "metadata": {},
   "source": [
    "### 1. Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c520ef",
   "metadata": {},
   "source": [
    "#### 1.0. Load and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dd94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = ''\n",
    "fins_train = ['eastasian_hate_sub_train.csv']\n",
    "fins_test = ['eastasian_hate_sub_test.csv']\n",
    "track = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2072e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply only this preprocessing because our data is already preprocessed\n",
    "def cleanNonAscii(text):\n",
    "    '''\n",
    "    Remove Non ASCII characters from the dataset.\n",
    "    Arguments:\n",
    "        text: str\n",
    "    returns: \n",
    "        text: str\n",
    "    '''\n",
    "    return ''.join(i for i in text if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2211e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; i can ’ t say what i want to say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; trust me . as hong konger , i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i ’ ve been living in china during the corona ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>those who like blaming &lt;hashtag&gt; chinese virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;hashtag&gt; china virus &lt;/hashtag&gt; china should ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  <user> <user> i can ’ t say what i want to say...\n",
       "1      1  <user> <user> trust me . as hong konger , i ca...\n",
       "2      0  i ’ ve been living in china during the corona ...\n",
       "3      1  those who like blaming <hashtag> chinese virus...\n",
       "4      1  <hashtag> china virus </hashtag> china should ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(BASE+fins_train[track])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005a1478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "label      \n",
       "0      3120\n",
       "1      3116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d6393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c82632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train['text'].values, df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513e86be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>with such damning evidence , the silence of &lt;u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; does &lt;allcaps&gt; cdc &lt;/allcaps&gt; ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;url&gt; &lt;hashtag&gt; chinazi virus &lt;/hashtag&gt; keeps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt; 2 4 hrs later . unimpressed . &lt;hashtag&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>rt &lt;user&gt; make sure you are prepared with the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  with such damning evidence , the silence of <u...\n",
       "1      0  <user> <user> does <allcaps> cdc </allcaps> ha...\n",
       "2      1  <url> <hashtag> chinazi virus </hashtag> keeps...\n",
       "3      0  <user> 2 4 hrs later . unimpressed . <hashtag>...\n",
       "4      0  rt <user> make sure you are prepared with the ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(BASE+fins_test[track])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa59fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(cleanNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd7faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_test['text'].values, df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9fcd878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<user> <user> i can  t say what i want to say about scaramucci but i will say how stupid he sounds , especially when he prefaces every ridiculous statement with ,  at the end of the day .  dumb . <hashtag> wuhan virus </hashtag> <hashtag> chinese coronavirus </hashtag>',\n",
       " 'with such damning evidence , the silence of <user> and the international community to call out <hashtag> china </hashtag>  s irresponsible actions that are threatening the world is not only baffling but points to something nefarious altogether . <hashtag> wuhan coronavirus </hashtag> <hashtag> china virus </hashtag> <hashtag> covid19 </hashtag> <url>')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61273054",
   "metadata": {},
   "source": [
    "#### 1.1. Transforming data suitable for model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0312b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "num_words = 100000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = max(map(lambda x: len(x),xtrain))\n",
    "xtrain = pad_sequences(xtrain, maxlen=maxlen)\n",
    "\n",
    "xtest = tokenizer.texts_to_sequences(X_test)\n",
    "xtest = pad_sequences(xtest, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51810ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6236, 95)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c9fc6",
   "metadata": {},
   "source": [
    "####  1.2. Dataset mapper and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9a7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetMapper(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3fec1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset mapper objects for train and test\n",
    "train = DatasetMapper(xtrain, y_train)\n",
    "test = DatasetMapper(xtest, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5575d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise dataloaders for train and test\n",
    "loader_train = DataLoader(train, batch_size=32)\n",
    "loader_test = DataLoader(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504d650",
   "metadata": {},
   "source": [
    "### 2. Define CNN model network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6bc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(in_channels=95, out_channels=128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=95, out_channels=256, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=95, out_channels=512, kernel_size=5)\n",
    "        \n",
    "        # flatten and concat conv1, conv2, conv3\n",
    "        self.fc1 = nn.Linear(in_features=(128+256+512), out_features = 256)\n",
    "        self.out = nn.Linear(in_features=256, out_features=1)\n",
    "        \n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (0) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (1) embedding layer (this layer assign a vector to each word index in t)\n",
    "        t = self.embeddings(t)\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        tri_gram = self.conv1(t)\n",
    "        tri_gram = F.relu(tri_gram)\n",
    "        tri_gram = F.max_pool1d(tri_gram, kernel_size=tri_gram.shape[2], stride=1) # GlobalMaxPool1d using MaxPool1d\n",
    "        tri_gram = self.dropout(tri_gram)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        four_gram = self.conv2(t)\n",
    "        four_gram = F.relu(four_gram)\n",
    "        four_gram = F.max_pool1d(four_gram, kernel_size=four_gram.shape[2], stride=1) # GlobalMaxPool1d using MaxPool1d\n",
    "        four_gram = self.dropout(four_gram)\n",
    "        \n",
    "        # (4) hidden conv layer\n",
    "        five_gram = self.conv3(t)\n",
    "        five_gram = F.relu(five_gram)\n",
    "        five_gram = F.max_pool1d(five_gram, kernel_size=five_gram.shape[2], stride=1) # GlobalMaxPool1d using MaxPool1d\n",
    "        five_gram = self.dropout(five_gram)\n",
    "        \n",
    "        # flatten and concat conv1, conv2, conv3\n",
    "        t = torch.cat((tri_gram.squeeze(dim=2), four_gram.squeeze(dim=2), five_gram.squeeze(dim=2)), dim=1)\n",
    "        \n",
    "        # (6) hidden linear layer\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dropout(t)\n",
    "        \n",
    "        # (7) output layer\n",
    "        t = self.out(t)\n",
    "        t = torch.sigmoid(t)\n",
    "        \n",
    "        return t.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c263246",
   "metadata": {},
   "source": [
    "#### 2.0. Cheak CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc079353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model = CnnModel(vocab_size=1000)\n",
    "# cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f11291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat random word indices in the vocabulary\n",
    "# data = torch.randint(0, 1000, (1, 16)) # arguments: start_range=0, end_range=1000, (batch_size, words_in_instance=16)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65e7e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "# out = cnn_model(data)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d9f3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0][127]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7512c71",
   "metadata": {},
   "source": [
    "### 3. Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b31c69",
   "metadata": {},
   "source": [
    "#### 3.1. Create CNN model and Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "780d5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CnnModel(vocab_size=100000)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0f45f",
   "metadata": {},
   "source": [
    "#### 3.2. Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abace9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuray(grand_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "   \n",
    "    # Gets frequency  of true positives and true negatives\n",
    "    # The threshold is 0.5\n",
    "    for true, pred in zip(grand_truth, predictions):\n",
    "        if (pred >= 0.5) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred < 0.5) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    # Return accuracy\n",
    "    return (true_positives+true_negatives) / len(grand_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e0ce6",
   "metadata": {},
   "source": [
    "#### 3.3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764fa4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<----epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5375240538806928 -------->\n",
      "<----epoch 1\n",
      "accuracy 0.6130532392559332 -------->\n",
      "<----epoch 2\n",
      "accuracy 0.6619627966645285 -------->\n",
      "<----epoch 3\n",
      "accuracy 0.6937139191789609 -------->\n",
      "<----epoch 4\n",
      "accuracy 0.7554522129570237 -------->\n",
      "<----epoch 5\n",
      "accuracy 0.7867222578576011 -------->\n",
      "<----epoch 6\n",
      "accuracy 0.818313021167415 -------->\n",
      "<----epoch 7\n",
      "accuracy 0.8412443874278384 -------->\n",
      "<----epoch 8\n",
      "accuracy 0.8632135984605517 -------->\n",
      "<----epoch 9\n",
      "accuracy 0.8948043617703656 -------->\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    # set the model in training mode\n",
    "    cnn_model.train()\n",
    "    predictions = []\n",
    "    print('<----epoch', epoch)\n",
    "    \n",
    "    # start training the batches\n",
    "    for x_batch, y_batch in loader_train:\n",
    "        y_batch = y_batch.type(torch.FloatTensor) # changing datatype to float tensor\n",
    "        x_batch = x_batch.type(torch.LongTensor)\n",
    "        y_pred = cnn_model(x_batch) # feed to the model\n",
    "        loss = binary_cross_entropy(y_pred, y_batch) # calcualte loss\n",
    "        optimizer.zero_grad() # clearn gradiants\n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # update parameters based on gradients\n",
    "        predictions += list(y_pred.detach().numpy())\n",
    "    \n",
    "    # evaluate for one epoch\n",
    "    accuracy = calculate_accuray(y_train, predictions)\n",
    "    print('accuracy', accuracy, '-------->')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939b326",
   "metadata": {},
   "source": [
    "### 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5eff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader_test):\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    # Starst evaluation phase\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader_test:\n",
    "            x_batch = x_batch.type(torch.LongTensor)\n",
    "            y_pred = model(x_batch)\n",
    "            predictions += list(y_pred.detach().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31220ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7205128205128205\n"
     ]
    }
   ],
   "source": [
    "test_predictions = evaluation(cnn_model, loader_test)\n",
    "test_accuracy = calculate_accuray(y_test, test_predictions)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7d23dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluateBinaryClassification Object Created\n",
      "\n",
      "Total Samples\t780\n",
      "Positive Samples\t380\n",
      "Negative Samples\t400\n",
      "True Positive\t240\n",
      "True Negative\t322\n",
      "False Positive\t78\n",
      "False Negative\t140\n",
      "Accuracy\t0.7205128205128205\n",
      "Precision\t0.7547169811320755\n",
      "Recall\t0.631578947368421\n",
      "F1 Measure\t0.6876790830945558\n",
      "Cohen Kappa Score\t0.4383670233848592\n",
      "Area Under Curve\t0.7182894736842105\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       400\n",
      "           1       0.75      0.63      0.69       380\n",
      "\n",
      "    accuracy                           0.72       780\n",
      "   macro avg       0.73      0.72      0.72       780\n",
      "weighted avg       0.73      0.72      0.72       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate_classification import EvaluateBinaryClassification\n",
    "\n",
    "ebc = EvaluateBinaryClassification(gnd_truths = y_test, predictions = [int(round(y)) for y in test_predictions])\n",
    "print(ebc.get_full_report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
