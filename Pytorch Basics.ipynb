{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's discuss the basics first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors & Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tensor is an n-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(3., requires_grad=True), tensor(4., requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensors.\n",
    "X = torch.tensor(2.)\n",
    "W = torch.tensor(3., requires_grad=True)\n",
    "b = torch.tensor(4., requires_grad=True)\n",
    "X, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = W * X + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we can automatically compute the derivative of y w.r.t. a tensor that has \"requires_grad set to True\" e.g. w and b here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dw: tensor(2.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dw:', W.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's solve a toy problem using toy data\n",
    "\n",
    "## Problem Formulation\n",
    "We'll create a model that predicts product sells for iphone (target variables) by looking at the average imcome, price and tax (input variables or features) in a country. \n",
    "\n",
    "## Model architecture:\n",
    "Our model architecture (or equation) is y = WX + b. \n",
    "This architecture will create a linear regression model, i.e. each target variable is estimated to be a weighted sum of the input variables (w*x), offset by some constant, known as a bias (b).\n",
    "\n",
    "## Objective: \n",
    "We want to find a suitable set of weights (W) and a bias (b) using the training data, to make accurate predictions (i.e. reduce loss in predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (salary in K, price in K, tax in K)\n",
    "inputs = np.array([[70., 0.1, 0.043], \n",
    "                   [90., 0.8, 0.064], \n",
    "                   [80., 0.13, 0.058], \n",
    "                   [100., 0.4, 0.037], \n",
    "                   [60., 0.9, 0.7]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets iphone sell number in K\n",
    "targets = np.array([5, 8, 12, 2, 1], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can work with only PyTorch tensors. Therefore, we need to convert inputs and targets to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0e+01 1.0e-01 4.3e-02]\n",
      " [9.0e+01 8.0e-01 6.4e-02]\n",
      " [8.0e+01 1.3e-01 5.8e-02]\n",
      " [1.0e+02 4.0e-01 3.7e-02]\n",
      " [6.0e+01 9.0e-01 7.0e-01]]\n",
      "[ 5.  8. 12.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "X = torch.from_numpy(inputs)\n",
    "y = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise weights and biases\n",
    "The weights and biases can be represented as matrices and initialized with random values. The first row of W and the first element of b are use to predict the first target variable i.e. iphone sell, and similarly the second row of W the second element of be are used for second target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7765, -0.1135,  0.7029]], requires_grad=True)\n",
      "tensor([0.4655], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "W = torch.randn(1, 3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is simply a function that performs a matrix multiplication of the input X and the weights W (transposed) and adds the bias b (replicated for each observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture\n",
    "def model(X):\n",
    "    return X @ W.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[54.8403],\n",
      "        [70.3058],\n",
      "        [62.6125],\n",
      "        [78.0974],\n",
      "        [47.4461]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  8., 12.,  2.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "We can compare the predictions with the actual targets, using the following method:\n",
    "1. Calculate the difference between the two matrices (preds and targets).\n",
    "2. Square all elements of the difference matrix to remove negative values.\n",
    "3. Calculate the average of the elements in the resulting matrix.\n",
    "\n",
    "The result is a single number, known as the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[54.8403],\n",
       "        [70.3058],\n",
       "        [62.6125],\n",
       "        [78.0974],\n",
       "        [47.4461]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting number is called the loss, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3390.0071, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradients\n",
    "In PyTorch, we can automatically compute the gradients, i.e. derivative of the loss w.r.t. to the weights and biases (because weights and biases have requires_grad set to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are stored in the .grad attribute of the tensors W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7765, -0.1135,  0.7029]], requires_grad=True)\n",
      "tensor([[9436.7383,   52.3043,   16.6160]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(W)\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4655], requires_grad=True)\n",
      "tensor([114.1208])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From calculus we know that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. \n",
    "\n",
    "If a gradient element is postive,\n",
    "1. increasing the element's value slightly will increase the loss.\n",
    "2. decreasing the element's value slightly will decrease the loss.\n",
    "\n",
    "If a gradient element is negative,\n",
    "1. increasing the element's value slightly will decrease the loss.\n",
    "2. decreasing the element's value slightly will increase the loss.\n",
    "\n",
    "Also, we know that the increase or decrease is proportional to the value of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    W -= W.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note, we have to reset the gradients to zero before moving forward, because PyTorch accumulates gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(W.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm\n",
    "Now we will adjust weights and biases using gradient descent. This requires reducing the loss using the gradient descent algorithm, which has the following steps:\n",
    "1. Generate predictions\n",
    "2. Calculate the loss\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "4. Adjust the weights and biases by subtracting a small quantity proportional to the gradient\n",
    "5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[48.2333],\n",
      "        [61.8112],\n",
      "        [55.0619],\n",
      "        [68.6593],\n",
      "        [41.7824]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2558.1218, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(preds, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    W -= W.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6002, -0.1145,  0.7026]], requires_grad=True)\n",
      "tensor([0.4634], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new weights and biases, the model should have a lower loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1931.3949, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(X)\n",
    "loss = mse(preds, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for multiple epochs\n",
    "To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each repetation with all training instances is called an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 2000 epochs\n",
    "for i in range(2000):\n",
    "    preds = model(X)\n",
    "    loss = mse(preds, y)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= W.grad * 1e-7\n",
    "        b -= b.grad * 1e-7\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.8275, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(X)\n",
    "loss = mse(preds, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7970],\n",
       "        [5.9643],\n",
       "        [5.4213],\n",
       "        [6.6095],\n",
       "        [4.5470]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.,  8., 12.,  2.,  1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print targets\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
