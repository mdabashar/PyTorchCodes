{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08c1e22",
   "metadata": {},
   "source": [
    "# PyTorch LSTM Explained for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c0d35",
   "metadata": {},
   "source": [
    "### 1. Let's first explore functionality of Embedding Layer and LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4aa7bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b608762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02ef99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input \n",
    "# [[vector of word indices in sentence1], \n",
    "#  [vector of word indices in sentence2], \n",
    "#  [vector of word indices in sentence3]]\n",
    "X = torch.tensor([[1,2, 12,34, 56,78, 90,80],\n",
    "                 [12,45, 99,67, 6,23, 77,82],\n",
    "                 [3,24, 6,99, 12,56, 21,22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198f00e",
   "metadata": {},
   "source": [
    "We first pass the input (3x8) through an embedding layer, \n",
    "because word embeddings are better at capturing context and \n",
    "are spatially more efficient than one-hot vector representations.\n",
    "\n",
    "In Pytorch, we can use the nn.Embedding module to create this layer, which takes the vocabulary size and desired word-vector length as input. You can optionally provide a padding index, to indicate the index of the padding element in the embedding matrix.\n",
    "\n",
    "In the following example, our vocabulary consists of 100 words, so our input to the embedding layer can only be from 0–100, and it returns us a 100x7 embedding matrix, with the 0th index representing our padding element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0b6aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer = nn.Embedding(100, 7, padding_idx=0)\n",
    "emb_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b96b5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In nn.LSTM \n",
    "# input_size: The number of expected features in the input x_n\n",
    "# hidden_size: The number of features in the hidden state h_n\n",
    "# num_layers: Number of recurrent layers.\n",
    "# batch_first: If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "lstm_layer = nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3e99fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = emb_layer(X)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb13545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (h_n, c_n) = lstm_layer(embedded) # Inputs: input, (h_0, c_0); input shape is (batch, seq, feature)\n",
    "# Outputs: output, (h_n, c_n); \n",
    "# output shape is (batch, seq, feature) and it contains of all hidden states in the sequence; \n",
    "# h_n shape is (num_layers * num_directions, batch, hidden_size) and it contains the final output\n",
    "# c_n shape is (num_layers * num_directions, batch, hidden_size) and it contains Cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b124a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b46eee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbceb85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae94e355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2647,  0.0256,  0.5710],\n",
       "         [-0.2057, -0.0083,  0.0182],\n",
       "         [ 0.1438, -0.0867,  0.2076]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf2272",
   "metadata": {},
   "source": [
    "### 2. Implement Multiclass Text Classification — Predicting ratings from review comments\n",
    "\n",
    "Problem Statement: Given an item’s review comment, predict the rating ( takes integer values from 1 to 5, 1 being worst and 5 being best)\n",
    "\n",
    "Dataset: I’ve used the following dataset from Kaggle:\n",
    "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ca2f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23486, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
       "0           0          767   33                      NaN   \n",
       "1           1         1080   34                      NaN   \n",
       "2           2         1077   60  Some major design flaws   \n",
       "3           3         1049   50         My favorite buy!   \n",
       "4           4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "df_reviews = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n",
    "print(df_reviews.shape)\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f70db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews[df_reviews['Review Text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25946eb1",
   "metadata": {},
   "source": [
    "#### Metric\n",
    "\n",
    "We usually take accuracy as our metric for most classification problems, however, ratings are ordered. If the actual value is 5 but the model predicts a 4, it is not considered as bad as predicting a 1. Hence, instead of going with accuracy, we choose RMSE — root mean squared error as our North Star metric. Also, rating prediction is a pretty hard problem, even for humans, so a prediction of being off by just 1 point or lesser is considered pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1e48b",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "As mentioned earlier, we need to convert our text into a numerical form that can be fed to our model as input. I’ve used spacy for tokenization after removing punctuation, special characters, and lower casing the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c6440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8b545",
   "metadata": {},
   "source": [
    "#### vocabulary to index mapping\n",
    "We count the number of occurrences of each token in our corpus and get rid of the ones that don’t occur too frequently.\n",
    "\n",
    "We then create a vocabulary to index mapping and encode our review text using this mapping. I’ve chosen the maximum length of any review to be 70 words because the average length of reviews was around 60. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4cac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in df_reviews.iterrows():\n",
    "    counts.update(tokenize(row['Review Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f256249",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counts, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00ddbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[545, 8, 67, 2343]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65c964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab[word] for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139d8f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 545,    8,    3,   67, 2343,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence('here is the an example')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6b692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/basharm/mypython/lib64/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X = df_reviews['Review Text'].apply(lambda x: np.array(encode_sentence(x))).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66fdf51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([254, 523,  12, 889,   6, 648,   6,  73,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0]),\n",
       "       8], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3ed8efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3, 2, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews['Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e30296de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_reviews['Rating'].apply(lambda x: int(x) - 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842bffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, ..., 2, 2, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ed26ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cd6e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([array([ 491,    2,    9,   95,   10,  154,   70,    2,    4,   43,  367,\n",
       "        124,    6,    9,   85,    8,  136,  121,   14,   37,    2,   28,\n",
       "        121,    2,    6,    5,   24,   19,  277,    5,   24,  289,    7,\n",
       "       2639,  128,    2,    3,  251, 1025,    8,   12,   24,   12,   24,\n",
       "        140,  671,   85,    2,   87,    8,  917,   71,    3,   50,    8,\n",
       "         78,    2,    3,  258,    8,   75,   11,  168,    2,  107,    3,\n",
       "        441, 1216,   32,   40]),\n",
       "       70], dtype=object),\n",
       "       array([array([   4,   57,  349,    9,   21,  144,    6,    4,  170,    5,   11,\n",
       "        168,    2,    3,  235,    8,  246,    2,    3,  101,    8,    0,\n",
       "          2,  115,    8,  316,    2,   95,    7,   28,  388,  192,    2,\n",
       "        161,    7,  578,  204,    2,  139,   84,    2,    4,   35,  113,\n",
       "          7,   12,   11,  240,    6,   67,   12,   11,  567,    2,    4,\n",
       "         35,  302,   24,   39,  323,    6,  265,   12,   18, 3303,   41,\n",
       "        127,    2,   27,   12]),\n",
       "       70], dtype=object),\n",
       "       array([array([   4,   45,  245,    9,   94,  463,  950,    6,   25,    5,    2,\n",
       "          3,   88,    8,   19,   89, 1434,    6,  813,   76,    4,  402,\n",
       "          2,   13,  864,  295,    2,    3,  275,    8,  260,  552,    2,\n",
       "         13,   23,  443,   49,   80,    2,    4,  232,    9,   94,   10,\n",
       "       1969,  159,    7,  337,   21,   14, 2322,    6,  245,  987,   16,\n",
       "        215,    2,    5,    8,  314,    7,  279,  204,    0,    0,    0,\n",
       "          0,    0,    0,    0]),\n",
       "       63], dtype=object),\n",
       "       ...,\n",
       "       array([array([6324,    2,    4,  336,   29, 2173,   10,  572,  230,    2, 1758,\n",
       "          2,    9,   30,  139,   28,   84,    2,  140, 1534,    2,    5,\n",
       "          8,  625,    2,    4,   47,  280,   44,  122,  141,   89,  173,\n",
       "         49,  108,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0]),\n",
       "       35], dtype=object),\n",
       "       array([array([   9,    8,   67,  636,   26,   85,    6,    4,   35,   19,  315,\n",
       "          5,  627,  219,   14,  164,    2,    4,   72,  171, 6790,    5,\n",
       "         15,    7,  249,    2,    5,   24,  138,   10,   27,    2,   53,\n",
       "         34,  306,   48,   17,   75, 1476,   42,  306,   86,   17, 1072,\n",
       "         98,    2, 1061,   40,    6,   81,  246,  111,  114,  544,   39,\n",
       "          3, 1152,   14,    3,  179,   57,  198,    7, 1073,    2,  659,\n",
       "        580,   16,  215,    0]),\n",
       "       69], dtype=object),\n",
       "       array([array([   4,   25,    9,   85,   13,   29,   12,  446,    2,   22, 2251,\n",
       "       1141,    2,    5,   24,   19,  749,    2,    5,  107,   81,  749,\n",
       "         14,    3,  158,    2,  265,    2,    4,   77,  272,   26,   85,\n",
       "        242,    3,  319, 2289,  111, 2404,    6,  893,  169,    2,  319,\n",
       "        749,  607, 2597, 1141,    2,    5,   24,   19,  633,    2,    4,\n",
       "         53,   26,  107, 3135,    5,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0]),\n",
       "       60], dtype=object)], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15584bc",
   "metadata": {},
   "source": [
    "#### Pytorch Dataset and DataLoader\n",
    "\n",
    "The dataset is quite straightforward because we’ve already stored our encodings in the input dataframe. We also output the length of the input sequence in each case, because we can have LSTMs that take variable-length sequences.\n",
    "\n",
    "DataLoader uses dataset to interate through batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39d80f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72476985",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7449bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=5000, shuffle=True, num_workers=0)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=5000, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6476167",
   "metadata": {},
   "source": [
    "### Pytorch training loop\n",
    "\n",
    "The training loop is pretty standard. I’ve used Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4917ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, (x, y, l) in enumerate(train_dl):\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            val_loss, val_acc, val_rmse = validation_metrics(model, valid_dl)\n",
    "            if i % 5 == 1:\n",
    "                print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "                \n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        X = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ff2d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8086658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        return self.linear(hn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd426db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8832c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.592, val loss 1.467, val accuracy 0.537, and val rmse 1.370\n",
      "train loss 1.271, val loss 1.264, val accuracy 0.552, and val rmse 1.381\n",
      "train loss 1.231, val loss 1.244, val accuracy 0.552, and val rmse 1.381\n",
      "train loss 1.232, val loss 1.222, val accuracy 0.552, and val rmse 1.381\n",
      "train loss 1.201, val loss 1.229, val accuracy 0.552, and val rmse 1.381\n",
      "train loss 1.209, val loss 1.219, val accuracy 0.552, and val rmse 1.381\n",
      "train loss 1.205, val loss 1.222, val accuracy 0.552, and val rmse 1.379\n",
      "train loss 1.189, val loss 1.217, val accuracy 0.552, and val rmse 1.378\n",
      "train loss 1.185, val loss 1.218, val accuracy 0.551, and val rmse 1.377\n",
      "train loss 1.174, val loss 1.217, val accuracy 0.548, and val rmse 1.370\n",
      "train loss 1.164, val loss 1.218, val accuracy 0.542, and val rmse 1.355\n",
      "train loss 1.144, val loss 1.209, val accuracy 0.546, and val rmse 1.333\n",
      "train loss 1.083, val loss 1.167, val accuracy 0.559, and val rmse 1.202\n",
      "train loss 1.063, val loss 1.133, val accuracy 0.569, and val rmse 1.100\n",
      "train loss 1.018, val loss 1.179, val accuracy 0.569, and val rmse 1.201\n",
      "train loss 1.033, val loss 1.156, val accuracy 0.537, and val rmse 1.147\n",
      "train loss 1.021, val loss 1.178, val accuracy 0.574, and val rmse 1.186\n",
      "train loss 1.000, val loss 1.156, val accuracy 0.543, and val rmse 1.127\n",
      "train loss 0.968, val loss 1.141, val accuracy 0.574, and val rmse 1.112\n",
      "train loss 0.935, val loss 1.108, val accuracy 0.565, and val rmse 1.045\n",
      "train loss 0.942, val loss 1.111, val accuracy 0.571, and val rmse 1.040\n",
      "train loss 0.906, val loss 1.115, val accuracy 0.570, and val rmse 1.022\n",
      "train loss 1.018, val loss 1.141, val accuracy 0.537, and val rmse 1.051\n",
      "train loss 0.975, val loss 1.165, val accuracy 0.557, and val rmse 1.140\n",
      "train loss 0.928, val loss 1.128, val accuracy 0.545, and val rmse 1.257\n",
      "train loss 0.926, val loss 1.135, val accuracy 0.541, and val rmse 1.087\n",
      "train loss 0.904, val loss 1.128, val accuracy 0.558, and val rmse 1.066\n",
      "train loss 0.891, val loss 1.142, val accuracy 0.528, and val rmse 1.075\n",
      "train loss 0.889, val loss 1.134, val accuracy 0.555, and val rmse 1.061\n",
      "train loss 0.866, val loss 1.125, val accuracy 0.568, and val rmse 1.042\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ced4ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.000, val loss 1.141, val accuracy 0.575, and val rmse 1.079\n",
      "train loss 0.898, val loss 1.106, val accuracy 0.558, and val rmse 1.030\n",
      "train loss 0.887, val loss 1.131, val accuracy 0.563, and val rmse 1.107\n",
      "train loss 0.831, val loss 1.100, val accuracy 0.571, and val rmse 1.029\n",
      "train loss 0.811, val loss 1.107, val accuracy 0.567, and val rmse 1.021\n",
      "train loss 0.782, val loss 1.114, val accuracy 0.579, and val rmse 1.002\n",
      "train loss 0.778, val loss 1.108, val accuracy 0.571, and val rmse 0.982\n",
      "train loss 0.762, val loss 1.134, val accuracy 0.576, and val rmse 1.010\n",
      "train loss 0.761, val loss 1.142, val accuracy 0.544, and val rmse 0.965\n",
      "train loss 0.744, val loss 1.121, val accuracy 0.588, and val rmse 0.969\n",
      "train loss 0.746, val loss 1.106, val accuracy 0.582, and val rmse 0.967\n",
      "train loss 0.753, val loss 1.105, val accuracy 0.580, and val rmse 0.966\n",
      "train loss 0.713, val loss 1.145, val accuracy 0.584, and val rmse 1.022\n",
      "train loss 0.702, val loss 1.124, val accuracy 0.574, and val rmse 0.954\n",
      "train loss 0.678, val loss 1.141, val accuracy 0.589, and val rmse 0.965\n",
      "train loss 0.665, val loss 1.144, val accuracy 0.579, and val rmse 0.955\n",
      "train loss 0.658, val loss 1.160, val accuracy 0.588, and val rmse 0.948\n",
      "train loss 0.656, val loss 1.164, val accuracy 0.573, and val rmse 0.969\n",
      "train loss 0.647, val loss 1.186, val accuracy 0.582, and val rmse 0.964\n",
      "train loss 0.641, val loss 1.188, val accuracy 0.582, and val rmse 0.947\n",
      "train loss 0.625, val loss 1.184, val accuracy 0.572, and val rmse 0.964\n",
      "train loss 0.622, val loss 1.213, val accuracy 0.576, and val rmse 1.026\n",
      "train loss 0.613, val loss 1.229, val accuracy 0.574, and val rmse 0.953\n",
      "train loss 0.622, val loss 1.207, val accuracy 0.575, and val rmse 0.947\n",
      "train loss 0.599, val loss 1.215, val accuracy 0.573, and val rmse 0.983\n",
      "train loss 0.598, val loss 1.233, val accuracy 0.573, and val rmse 0.955\n",
      "train loss 0.600, val loss 1.251, val accuracy 0.589, and val rmse 0.971\n",
      "train loss 0.608, val loss 1.252, val accuracy 0.564, and val rmse 1.014\n",
      "train loss 0.602, val loss 1.257, val accuracy 0.573, and val rmse 1.002\n",
      "train loss 0.591, val loss 1.269, val accuracy 0.565, and val rmse 0.974\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "640b3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.827, val loss 1.262, val accuracy 0.582, and val rmse 1.033\n",
      "train loss 0.660, val loss 1.220, val accuracy 0.571, and val rmse 1.002\n",
      "train loss 0.618, val loss 1.239, val accuracy 0.581, and val rmse 1.002\n",
      "train loss 0.562, val loss 1.261, val accuracy 0.564, and val rmse 0.956\n",
      "train loss 0.546, val loss 1.263, val accuracy 0.564, and val rmse 0.959\n",
      "train loss 0.541, val loss 1.279, val accuracy 0.577, and val rmse 0.969\n",
      "train loss 0.521, val loss 1.295, val accuracy 0.565, and val rmse 0.950\n",
      "train loss 0.494, val loss 1.311, val accuracy 0.569, and val rmse 0.954\n",
      "train loss 0.483, val loss 1.328, val accuracy 0.576, and val rmse 0.948\n",
      "train loss 0.478, val loss 1.341, val accuracy 0.574, and val rmse 0.964\n",
      "train loss 0.464, val loss 1.352, val accuracy 0.580, and val rmse 0.960\n",
      "train loss 0.465, val loss 1.384, val accuracy 0.561, and val rmse 0.972\n",
      "train loss 0.467, val loss 1.370, val accuracy 0.578, and val rmse 0.979\n",
      "train loss 0.454, val loss 1.417, val accuracy 0.550, and val rmse 0.960\n",
      "train loss 0.457, val loss 1.415, val accuracy 0.577, and val rmse 0.973\n",
      "train loss 0.437, val loss 1.436, val accuracy 0.569, and val rmse 0.962\n",
      "train loss 0.433, val loss 1.435, val accuracy 0.571, and val rmse 0.965\n",
      "train loss 0.435, val loss 1.461, val accuracy 0.565, and val rmse 0.975\n",
      "train loss 0.457, val loss 1.463, val accuracy 0.583, and val rmse 0.985\n",
      "train loss 0.435, val loss 1.505, val accuracy 0.569, and val rmse 0.976\n",
      "train loss 0.415, val loss 1.492, val accuracy 0.581, and val rmse 0.978\n",
      "train loss 0.404, val loss 1.524, val accuracy 0.570, and val rmse 0.977\n",
      "train loss 0.396, val loss 1.532, val accuracy 0.575, and val rmse 0.973\n",
      "train loss 0.373, val loss 1.542, val accuracy 0.573, and val rmse 0.978\n",
      "train loss 0.369, val loss 1.572, val accuracy 0.570, and val rmse 0.978\n",
      "train loss 0.375, val loss 1.561, val accuracy 0.576, and val rmse 0.981\n",
      "train loss 0.370, val loss 1.575, val accuracy 0.573, and val rmse 0.979\n",
      "train loss 0.358, val loss 1.604, val accuracy 0.570, and val rmse 0.974\n",
      "train loss 0.344, val loss 1.597, val accuracy 0.575, and val rmse 0.976\n",
      "train loss 0.327, val loss 1.641, val accuracy 0.571, and val rmse 0.973\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff547a",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
