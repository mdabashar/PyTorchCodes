{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08c1e22",
   "metadata": {},
   "source": [
    "# PyTorch LSTM Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c0d35",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa7bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b608762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef47788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2      3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4      3  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "texts = []\n",
    "labels = []\n",
    "for (label, text) in train_iter:\n",
    "    labels.append(label)\n",
    "    texts.append(text)\n",
    "df_train = pd.DataFrame({'label': labels, 'text': texts})\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512b7278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks Unions repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      3  Fears for T N pension after talks Unions repre...\n",
       "1      4  The Race is On: Second Private Team Sets Launc...\n",
       "2      4  Ky. Company Wins Grant to Study Peptides (AP) ...\n",
       "3      4  Prediction Unit Helps Forecast Wildfires (AP) ...\n",
       "4      4  Calif. Aims to Limit Farm-Related Smog (AP) AP..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter = AG_NEWS(split='test')\n",
    "texts = []\n",
    "labels = []\n",
    "for (label, text) in test_iter:\n",
    "    labels.append(label)\n",
    "    texts.append(text)\n",
    "df_test = pd.DataFrame({'label': labels, 'text': texts})\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1e48b",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "As mentioned earlier, we need to convert our text into a numerical form that can be fed to our model as input. I’ve used spacy for tokenization after removing punctuation, special characters, and lower casing the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c6440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tokenization\n",
    "# tok = spacy.load('en_core_web_sm')\n",
    "# def tokenize(text):\n",
    "#     #text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "#     #regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "#     #nopunct = regex.sub(\" \", text.lower())\n",
    "#     return [token.text for token in tok.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7007a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenize = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8b545",
   "metadata": {},
   "source": [
    "#### Vocabulary to index mapping\n",
    "We count the number of occurrences of each token in our corpus and get rid of the ones that don’t occur too frequently.\n",
    "\n",
    "We then create a vocabulary to index mapping and encode our review text using this mapping. I’ve chosen the maximum length of any review to be 70 words because the average length of reviews was around 60. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4cac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in df_train.iterrows():\n",
    "    counts.update(tokenize(row['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f256249",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counts, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00ddbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 31, 5298]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65c964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab[word] for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139d8f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 476,   22,    3,   31, 5298,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]),\n",
       " 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e6b692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/basharm/mypython/lib64/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text'].apply(lambda x: np.array(encode_sentence(x))).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fdf51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,\n",
       "          14,    28,    15,    28,    16, 50726,     4,   432,   375,\n",
       "          17,    10, 67508,     7, 52259,     4,    43,  4010,   784,\n",
       "         326,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0]),\n",
       "       29], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ed8efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e30296de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label'].apply(lambda x: int(x) - 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842bffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed26ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/basharm/mypython/lib64/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test['text'].apply(lambda x: np.array(encode_sentence(x))).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8b6071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([  870,    12,    84,   138,  1482,    35,   174,  1753,  4059,\n",
       "         401,    21,  6558, 38435,   234,    68,    43,    17,  4478,\n",
       "          17,    35,   174,    19, 11302,  2448,   321,   195,  9840,\n",
       "           2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0]),\n",
       "       28], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ee4c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea828af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['label'].apply(lambda x: int(x) - 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15584bc",
   "metadata": {},
   "source": [
    "#### Pytorch Dataset and DataLoader\n",
    "\n",
    "The dataset is quite straightforward because we’ve already stored our encodings in the input dataframe. We also output the length of the input sequence in each case, because we can have LSTMs that take variable-length sequences.\n",
    "\n",
    "DataLoader uses dataset to interate through batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d80f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72476985",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NewsDataset(X_train, y_train)\n",
    "valid_ds = NewsDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7449bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=5000, shuffle=True, num_workers=0)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=5000, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6476167",
   "metadata": {},
   "source": [
    "### Pytorch training loop\n",
    "\n",
    "The training loop is pretty standard. I’ve used Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4917ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, (x, y, l) in enumerate(train_dl):\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            val_loss, val_acc, val_rmse = validation_metrics(model, valid_dl)\n",
    "            if i % 5 == 1:\n",
    "                print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "                \n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        X = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8086658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        return self.linear(hn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd426db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model_fixed =  LSTM_fixed_len(vocab_size, embedding_dim=100, hidden_dim=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8832c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.602, val loss 1.524, val accuracy 0.251, and val rmse 1.855\n",
      "train loss 1.475, val loss 1.395, val accuracy 0.249, and val rmse 1.231\n",
      "train loss 1.440, val loss 1.389, val accuracy 0.250, and val rmse 1.236\n",
      "train loss 1.425, val loss 1.389, val accuracy 0.252, and val rmse 1.867\n",
      "train loss 1.416, val loss 1.387, val accuracy 0.254, and val rmse 1.859\n",
      "train loss 1.387, val loss 1.385, val accuracy 0.253, and val rmse 1.860\n",
      "train loss 1.385, val loss 1.383, val accuracy 0.258, and val rmse 1.226\n",
      "train loss 1.384, val loss 1.382, val accuracy 0.259, and val rmse 1.850\n",
      "train loss 1.379, val loss 1.356, val accuracy 0.298, and val rmse 1.840\n",
      "train loss 1.366, val loss 1.272, val accuracy 0.412, and val rmse 1.187\n",
      "train loss 1.209, val loss 1.181, val accuracy 0.437, and val rmse 1.390\n",
      "train loss 1.155, val loss 1.113, val accuracy 0.465, and val rmse 1.119\n",
      "train loss 1.120, val loss 1.070, val accuracy 0.489, and val rmse 1.092\n",
      "train loss 1.095, val loss 0.999, val accuracy 0.537, and val rmse 1.148\n",
      "train loss 1.065, val loss 0.913, val accuracy 0.612, and val rmse 1.133\n",
      "train loss 0.843, val loss 0.826, val accuracy 0.653, and val rmse 0.961\n",
      "train loss 0.767, val loss 0.692, val accuracy 0.747, and val rmse 0.868\n",
      "train loss 0.699, val loss 0.569, val accuracy 0.801, and val rmse 0.809\n",
      "train loss 0.641, val loss 0.487, val accuracy 0.839, and val rmse 0.744\n",
      "train loss 0.594, val loss 0.440, val accuracy 0.855, and val rmse 0.692\n",
      "train loss 0.343, val loss 0.404, val accuracy 0.867, and val rmse 0.658\n",
      "train loss 0.316, val loss 0.386, val accuracy 0.872, and val rmse 0.647\n",
      "train loss 0.310, val loss 0.363, val accuracy 0.878, and val rmse 0.631\n",
      "train loss 0.302, val loss 0.352, val accuracy 0.888, and val rmse 0.603\n",
      "train loss 0.296, val loss 0.332, val accuracy 0.893, and val rmse 0.583\n",
      "train loss 0.211, val loss 0.329, val accuracy 0.891, and val rmse 0.594\n",
      "train loss 0.206, val loss 0.335, val accuracy 0.896, and val rmse 0.567\n",
      "train loss 0.204, val loss 0.319, val accuracy 0.898, and val rmse 0.574\n",
      "train loss 0.201, val loss 0.316, val accuracy 0.899, and val rmse 0.567\n",
      "train loss 0.201, val loss 0.321, val accuracy 0.896, and val rmse 0.583\n",
      "train loss 0.163, val loss 0.321, val accuracy 0.897, and val rmse 0.580\n",
      "train loss 0.153, val loss 0.327, val accuracy 0.900, and val rmse 0.562\n",
      "train loss 0.148, val loss 0.322, val accuracy 0.900, and val rmse 0.559\n",
      "train loss 0.147, val loss 0.318, val accuracy 0.899, and val rmse 0.571\n",
      "train loss 0.145, val loss 0.320, val accuracy 0.900, and val rmse 0.574\n",
      "train loss 0.111, val loss 0.318, val accuracy 0.900, and val rmse 0.550\n",
      "train loss 0.106, val loss 0.339, val accuracy 0.901, and val rmse 0.556\n",
      "train loss 0.106, val loss 0.344, val accuracy 0.899, and val rmse 0.564\n",
      "train loss 0.104, val loss 0.347, val accuracy 0.900, and val rmse 0.566\n",
      "train loss 0.105, val loss 0.347, val accuracy 0.900, and val rmse 0.562\n",
      "train loss 0.076, val loss 0.346, val accuracy 0.899, and val rmse 0.573\n",
      "train loss 0.081, val loss 0.360, val accuracy 0.901, and val rmse 0.564\n",
      "train loss 0.079, val loss 0.363, val accuracy 0.903, and val rmse 0.557\n",
      "train loss 0.080, val loss 0.359, val accuracy 0.901, and val rmse 0.568\n",
      "train loss 0.080, val loss 0.363, val accuracy 0.899, and val rmse 0.572\n",
      "train loss 0.054, val loss 0.377, val accuracy 0.899, and val rmse 0.581\n",
      "train loss 0.059, val loss 0.384, val accuracy 0.899, and val rmse 0.568\n",
      "train loss 0.062, val loss 0.386, val accuracy 0.899, and val rmse 0.573\n",
      "train loss 0.063, val loss 0.376, val accuracy 0.901, and val rmse 0.565\n",
      "train loss 0.062, val loss 0.382, val accuracy 0.897, and val rmse 0.572\n",
      "train loss 0.049, val loss 0.401, val accuracy 0.899, and val rmse 0.559\n",
      "train loss 0.047, val loss 0.421, val accuracy 0.897, and val rmse 0.570\n",
      "train loss 0.047, val loss 0.419, val accuracy 0.896, and val rmse 0.581\n",
      "train loss 0.048, val loss 0.407, val accuracy 0.900, and val rmse 0.571\n",
      "train loss 0.049, val loss 0.402, val accuracy 0.898, and val rmse 0.569\n",
      "train loss 0.040, val loss 0.414, val accuracy 0.898, and val rmse 0.573\n",
      "train loss 0.040, val loss 0.437, val accuracy 0.901, and val rmse 0.564\n",
      "train loss 0.040, val loss 0.437, val accuracy 0.896, and val rmse 0.596\n",
      "train loss 0.042, val loss 0.425, val accuracy 0.901, and val rmse 0.562\n",
      "train loss 0.043, val loss 0.416, val accuracy 0.899, and val rmse 0.574\n",
      "train loss 0.037, val loss 0.428, val accuracy 0.903, and val rmse 0.562\n",
      "train loss 0.036, val loss 0.446, val accuracy 0.897, and val rmse 0.572\n",
      "train loss 0.037, val loss 0.447, val accuracy 0.902, and val rmse 0.558\n",
      "train loss 0.037, val loss 0.438, val accuracy 0.900, and val rmse 0.560\n",
      "train loss 0.038, val loss 0.454, val accuracy 0.896, and val rmse 0.578\n",
      "train loss 0.032, val loss 0.456, val accuracy 0.899, and val rmse 0.573\n",
      "train loss 0.031, val loss 0.459, val accuracy 0.895, and val rmse 0.583\n",
      "train loss 0.032, val loss 0.448, val accuracy 0.896, and val rmse 0.585\n",
      "train loss 0.033, val loss 0.461, val accuracy 0.896, and val rmse 0.577\n",
      "train loss 0.034, val loss 0.478, val accuracy 0.897, and val rmse 0.583\n",
      "train loss 0.023, val loss 0.463, val accuracy 0.895, and val rmse 0.581\n",
      "train loss 0.028, val loss 0.477, val accuracy 0.894, and val rmse 0.583\n",
      "train loss 0.029, val loss 0.485, val accuracy 0.892, and val rmse 0.582\n",
      "train loss 0.028, val loss 0.495, val accuracy 0.894, and val rmse 0.588\n",
      "train loss 0.029, val loss 0.492, val accuracy 0.894, and val rmse 0.580\n",
      "train loss 0.019, val loss 0.495, val accuracy 0.891, and val rmse 0.608\n",
      "train loss 0.021, val loss 0.506, val accuracy 0.895, and val rmse 0.582\n",
      "train loss 0.022, val loss 0.518, val accuracy 0.894, and val rmse 0.590\n",
      "train loss 0.023, val loss 0.503, val accuracy 0.894, and val rmse 0.591\n",
      "train loss 0.024, val loss 0.517, val accuracy 0.895, and val rmse 0.593\n",
      "train loss 0.017, val loss 0.513, val accuracy 0.891, and val rmse 0.607\n",
      "train loss 0.018, val loss 0.537, val accuracy 0.895, and val rmse 0.589\n",
      "train loss 0.019, val loss 0.523, val accuracy 0.894, and val rmse 0.586\n",
      "train loss 0.020, val loss 0.500, val accuracy 0.897, and val rmse 0.585\n",
      "train loss 0.022, val loss 0.499, val accuracy 0.897, and val rmse 0.581\n",
      "train loss 0.014, val loss 0.517, val accuracy 0.896, and val rmse 0.585\n",
      "train loss 0.019, val loss 0.508, val accuracy 0.895, and val rmse 0.583\n",
      "train loss 0.021, val loss 0.498, val accuracy 0.896, and val rmse 0.580\n",
      "train loss 0.021, val loss 0.508, val accuracy 0.896, and val rmse 0.577\n",
      "train loss 0.022, val loss 0.522, val accuracy 0.896, and val rmse 0.570\n",
      "train loss 0.019, val loss 0.531, val accuracy 0.896, and val rmse 0.590\n",
      "train loss 0.017, val loss 0.527, val accuracy 0.895, and val rmse 0.593\n",
      "train loss 0.017, val loss 0.543, val accuracy 0.897, and val rmse 0.592\n",
      "train loss 0.017, val loss 0.551, val accuracy 0.895, and val rmse 0.592\n",
      "train loss 0.018, val loss 0.533, val accuracy 0.896, and val rmse 0.594\n",
      "train loss 0.013, val loss 0.533, val accuracy 0.897, and val rmse 0.583\n",
      "train loss 0.015, val loss 0.530, val accuracy 0.896, and val rmse 0.587\n",
      "train loss 0.015, val loss 0.560, val accuracy 0.894, and val rmse 0.588\n",
      "train loss 0.015, val loss 0.590, val accuracy 0.897, and val rmse 0.579\n",
      "train loss 0.015, val loss 0.564, val accuracy 0.894, and val rmse 0.588\n",
      "train loss 0.011, val loss 0.562, val accuracy 0.896, and val rmse 0.585\n",
      "train loss 0.013, val loss 0.570, val accuracy 0.895, and val rmse 0.596\n",
      "train loss 0.012, val loss 0.589, val accuracy 0.895, and val rmse 0.596\n",
      "train loss 0.013, val loss 0.589, val accuracy 0.894, and val rmse 0.591\n",
      "train loss 0.013, val loss 0.566, val accuracy 0.894, and val rmse 0.589\n",
      "train loss 0.011, val loss 0.575, val accuracy 0.891, and val rmse 0.597\n",
      "train loss 0.011, val loss 0.597, val accuracy 0.892, and val rmse 0.595\n",
      "train loss 0.011, val loss 0.627, val accuracy 0.888, and val rmse 0.617\n",
      "train loss 0.012, val loss 0.615, val accuracy 0.894, and val rmse 0.588\n",
      "train loss 0.013, val loss 0.595, val accuracy 0.889, and val rmse 0.594\n",
      "train loss 0.010, val loss 0.582, val accuracy 0.894, and val rmse 0.592\n",
      "train loss 0.011, val loss 0.614, val accuracy 0.889, and val rmse 0.602\n",
      "train loss 0.011, val loss 0.618, val accuracy 0.891, and val rmse 0.600\n",
      "train loss 0.012, val loss 0.621, val accuracy 0.887, and val rmse 0.610\n",
      "train loss 0.012, val loss 0.584, val accuracy 0.890, and val rmse 0.610\n",
      "train loss 0.009, val loss 0.569, val accuracy 0.891, and val rmse 0.601\n",
      "train loss 0.011, val loss 0.590, val accuracy 0.889, and val rmse 0.610\n",
      "train loss 0.012, val loss 0.601, val accuracy 0.892, and val rmse 0.599\n",
      "train loss 0.012, val loss 0.608, val accuracy 0.889, and val rmse 0.595\n",
      "train loss 0.012, val loss 0.596, val accuracy 0.890, and val rmse 0.584\n",
      "train loss 0.006, val loss 0.591, val accuracy 0.891, and val rmse 0.598\n",
      "train loss 0.008, val loss 0.611, val accuracy 0.893, and val rmse 0.595\n",
      "train loss 0.009, val loss 0.613, val accuracy 0.890, and val rmse 0.593\n",
      "train loss 0.010, val loss 0.610, val accuracy 0.891, and val rmse 0.599\n",
      "train loss 0.011, val loss 0.586, val accuracy 0.892, and val rmse 0.595\n",
      "train loss 0.011, val loss 0.580, val accuracy 0.894, and val rmse 0.591\n",
      "train loss 0.009, val loss 0.587, val accuracy 0.896, and val rmse 0.581\n",
      "train loss 0.009, val loss 0.583, val accuracy 0.895, and val rmse 0.587\n",
      "train loss 0.009, val loss 0.598, val accuracy 0.895, and val rmse 0.590\n",
      "train loss 0.010, val loss 0.613, val accuracy 0.895, and val rmse 0.592\n",
      "train loss 0.009, val loss 0.604, val accuracy 0.897, and val rmse 0.597\n",
      "train loss 0.009, val loss 0.609, val accuracy 0.898, and val rmse 0.588\n",
      "train loss 0.009, val loss 0.616, val accuracy 0.895, and val rmse 0.601\n",
      "train loss 0.009, val loss 0.619, val accuracy 0.895, and val rmse 0.597\n",
      "train loss 0.009, val loss 0.613, val accuracy 0.894, and val rmse 0.598\n",
      "train loss 0.009, val loss 0.588, val accuracy 0.896, and val rmse 0.587\n",
      "train loss 0.009, val loss 0.586, val accuracy 0.896, and val rmse 0.580\n",
      "train loss 0.009, val loss 0.592, val accuracy 0.896, and val rmse 0.584\n",
      "train loss 0.009, val loss 0.603, val accuracy 0.898, and val rmse 0.575\n",
      "train loss 0.010, val loss 0.593, val accuracy 0.896, and val rmse 0.582\n",
      "train loss 0.006, val loss 0.602, val accuracy 0.896, and val rmse 0.577\n",
      "train loss 0.007, val loss 0.612, val accuracy 0.893, and val rmse 0.593\n",
      "train loss 0.008, val loss 0.606, val accuracy 0.895, and val rmse 0.583\n",
      "train loss 0.009, val loss 0.590, val accuracy 0.894, and val rmse 0.585\n",
      "train loss 0.010, val loss 0.595, val accuracy 0.897, and val rmse 0.573\n",
      "train loss 0.006, val loss 0.583, val accuracy 0.896, and val rmse 0.580\n",
      "train loss 0.006, val loss 0.613, val accuracy 0.895, and val rmse 0.583\n",
      "train loss 0.008, val loss 0.635, val accuracy 0.897, and val rmse 0.582\n",
      "train loss 0.008, val loss 0.654, val accuracy 0.892, and val rmse 0.594\n",
      "train loss 0.009, val loss 0.646, val accuracy 0.894, and val rmse 0.583\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ced4ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640b3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff547a",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
